# Datasets Acquisition Toolkit

æœ¬ç›®å½• (`./scripts/datasets/`) æä¾›ç»Ÿä¸€çš„æ•°æ®é›†è·å–ä¸è½åœ°å·¥å…·ï¼Œé…åˆ YAML é…ç½®é©±åŠ¨ï¼Œä¿è¯å›¢é˜Ÿå®éªŒçš„ **å¯å¤ç°æ€§**ã€**å¯æ‰©å±•æ€§** ä¸ **å®‰å…¨æ€§**ã€‚

---

## ğŸ“‚ ç›®å½•ç»“æ„

```
./scripts/datasets/
  â”œâ”€ acquire_datasets.py   # CLI ä¸»å…¥å£ (YAML é©±åŠ¨)
  â”œâ”€ configs/              # æ•°æ®é›†è§„æ ¼é…ç½®ç›®å½•
  â”‚    â”œâ”€ base.yaml        # å…¨å±€ç¼ºçœé…ç½®
  â”‚    â””â”€ <name>.yaml     # å•ä¸ªæ•°æ®é›†è§„æ ¼
```

ä¸‹è½½åçš„æ•°æ®ç»Ÿä¸€è½åœ¨ä»“åº“æ ¹ç›®å½•ä¸‹ï¼š

```
./datasets/
  â””â”€ <name>/
      â”œâ”€ raw/         # åŸå§‹ä¸‹è½½ç‰© (zip/tar/json/csv/...)
      â”œâ”€ extracted/   # è§£å‹åçš„æ–‡ä»¶æ ‘
      â”œâ”€ processed/   # è§„èŒƒåŒ–åçš„æ–‡ä»¶ (parquet/csv/jsonl)
      â””â”€ dataset_card.json  # å…ƒä¿¡æ¯ä¸è¿½æº¯å¿«ç…§
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. åˆ—å‡ºå¯ç”¨æ•°æ®é›†

```bash
python scripts/datasets/acquire_datasets.py list
```

> ä» `configs/` ç›®å½•è¯»å–æ‰€æœ‰ `<name>.yaml` é…ç½®ã€‚

### 2. æŸ¥çœ‹æŸä¸ªæ•°æ®é›†é…ç½®

```bash
python scripts/datasets/acquire_datasets.py show liar
```

> æ‰“å°åˆå¹¶åçš„é…ç½® (base.yaml + liar.yaml + CLI è¦†ç›–)ã€‚

### 3. ä¸‹è½½æ•°æ®é›†

```bash
python scripts/datasets/acquire_datasets.py download liar --format parquet
```

* é»˜è®¤å¯¼å‡ºæ ¼å¼æ¥è‡ª `base.yaml` çš„ `format_hint`ã€‚
* æ”¯æŒ `--force` è¦†ç›–å·²å­˜åœ¨ç›®å½•ã€‚

### 4. æ ¡éªŒ URL æº

```bash
python scripts/datasets/acquire_datasets.py verify politifact
```

> å¯¹ `source=url` ä¸”æä¾›äº† `checksums` çš„æ•°æ®é›†è¿›è¡Œ sha256 æ ¡éªŒã€‚

---

## ğŸ“ é…ç½®è§„èŒƒ (YAML Schema)

æ‰€æœ‰é…ç½®æ–‡ä»¶æœ€ç»ˆä¼šä¸ `base.yaml` åˆå¹¶ï¼Œå­—æ®µçº§è¦†ç›–ï¼š

* ç©ºå€¼ **ä¸ä¼šè¦†ç›–** éç©ºå€¼ã€‚
* ä¼˜å…ˆçº§ï¼š`base.yaml` < `<name>.yaml` < CLI `--spec-file/--spec-json`ã€‚

### å¿…å¡«å­—æ®µ

* `name`: æ•°æ®é›†å (ç›®å½•å)
* `source`: æ•°æ®æºç±»å‹ (`hf` | `url`)

### å¯é€‰å­—æ®µ

* `hf_id`: HuggingFace æ•°æ®é›† ID
* `subset`: HuggingFace å­é›†å
* `urls`: URL åˆ—è¡¨ (å½“ `source=url` æ—¶å¿…å¡«)
* `checksums`: æ–‡ä»¶å -> sha256 æ ¡éªŒå€¼
* `format_hint`: parquet | csv | jsonl
* `license`: è®¸å¯ä¿¡æ¯
* `provenance`: æ¥æº (homepage/paper/revision)
* `tags`: æ ‡ç­¾åˆ—è¡¨
* `notes`: å¤‡æ³¨

### ç¤ºä¾‹

```yaml
# configs/liar.yaml
schema_version: 1
name: liar
source: hf
hf_id: liar
license: CC BY-NC 4.0
provenance:
  homepage: https://huggingface.co/datasets/liar
  paper: https://arxiv.org/abs/1705.00648
tags: [fake-news, claim-verification]
notes: "LIAR via HuggingFace"
```

---

## ğŸ”’ å®‰å…¨ä¸è¿½æº¯

* è§£å‹é‡‡ç”¨ **Zip Slip é˜²æŠ¤**ï¼Œä¿è¯æ–‡ä»¶ä»…é‡Šæ”¾åˆ° `extracted/` å­ç›®å½•ã€‚
* `dataset_card.json` å†…åŒ…å«ï¼š

  * `spec_snapshot` (åˆå¹¶åçš„é…ç½®å¿«ç…§)
  * `spec_hash` (å¿«ç…§å“ˆå¸Œ)
  * `driver` (ä½¿ç”¨çš„é©±åŠ¨)
  * `command_line` (è¿è¡Œå‘½ä»¤)
  * `created_at` æ—¶é—´æˆ³

---

## ğŸ“¦ ä¾èµ–

è¯¦è§ä»“åº“æ ¹ç›®å½•ä¸‹çš„ `requirements.txt`ï¼š

* [datasets](https://github.com/huggingface/datasets)
* pandas, pyarrow
* requests, tqdm
* PyYAML

å®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirements.txt
```

---

## ğŸ¾ æ—¥å¿—

* å·¥å…·ä¼šä¼˜å…ˆè°ƒç”¨é¡¹ç›®å†…çš„é›†ä¸­å¼æ—¥å¿—æ¨¡å— `kan/utils/logging.py`ï¼Œæ³¨å…¥ `run_id/stage/step` ä¸Šä¸‹æ–‡ï¼Œè¾“å‡ºåˆ°æ§åˆ¶å°ä¸è½®è½¬æ—¥å¿—æ–‡ä»¶ã€‚
* å¦‚æœè¯¥æ¨¡å—ä¸å¯ç”¨ï¼Œåˆ™å›é€€æ ‡å‡†åº“ loggingã€‚

---

## ğŸ“Œ å°ç»“

* é…ç½®ä¸æµç¨‹å½»åº•è§£è€¦ï¼Œæ–°å¢æ•°æ®é›†åªéœ€åœ¨ `configs/` ä¸‹æ·»åŠ  `<name>.yaml`ã€‚
* å…¼å®¹æ—§ CLI å‚æ•° (`--spec-json/--spec-file`)ã€‚
* å®‰å…¨å¯è¿½æº¯ï¼Œé€‚åˆç§‘ç ”ä¸ç”Ÿäº§å¤ç°ã€‚
