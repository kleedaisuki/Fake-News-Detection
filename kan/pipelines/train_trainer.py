# -*- coding: utf-8 -*-
from __future__ import annotations

"""
@file   kan/pipelines/train_trainer.py
@brief  Pipeline: ä½¿ç”¨ ğŸ¤— Transformers `Trainer` è®­ç»ƒ KANï¼ˆå¯åˆ‡æ¢ä¸ºä»…æ–‡æœ¬ / æ–‡æœ¬+çŸ¥è¯†ï¼‰ã€‚
@date   2025-09-16

@zh
  æœ¬æ–‡ä»¶å®šä¹‰ **æµç¨‹ï¼ˆpipelineï¼‰**ï¼Œè€Œéå…·ä½“æ¨¡å—å®ç°ï¼š
  - ä» `configs/` æœé›†å¹¶åˆå¹¶è¶…å‚æ•°ï¼ˆYAML + CLI overridesï¼‰ã€‚
  - ç»„ç»‡æ•°æ®åŠ è½½ï¼ˆkan.data.loadersï¼‰â†’ æ‰“åŒ…æ‰¹æ¬¡ï¼ˆkan.data.batcherï¼‰ã€‚
  - æ„å»ºç¼–ç å™¨/æ³¨æ„åŠ›/Headï¼ˆkan.modules.*ï¼‰å¹¶å°è£…ä¸ºå¯è®­ç»ƒçš„ nn.Moduleã€‚
  - ç”¨ ğŸ¤— `Trainer` è®­ç»ƒ/è¯„ä¼°ï¼Œç»“æœå†™å…¥ `runs/<run_id>/...`ï¼Œä¸­é—´ç¼“å­˜å†™å…¥ `cache/`ã€‚

@en
  This is a **pipeline**, not a concrete module implementation. It:
  - Merges configs from YAML files and CLI overrides.
  - Loads data via `kan.data.loaders` and batches via `kan.data.batcher`.
  - Builds encoders/attentions/head (kan.modules.*) and wraps them into a trainable nn.Module.
  - Trains/Evaluates using ğŸ¤— `Trainer`, writing artifacts to `runs/<run_id>/...` and caches to `cache/`.
"""

import argparse
import copy
import dataclasses
import json
import os
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import (
    Any,
    Dict,
    Iterable,
    List,
    Mapping,
    MutableMapping,
    Optional,
    Sequence,
    Tuple,
)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset

try:
    from transformers import (
        AutoTokenizer,
        HfArgumentParser,
        PreTrainedTokenizerBase,
        Trainer,
        TrainingArguments,
        set_seed as hf_set_seed,
    )
except Exception as e:  # pragma: no cover
    raise RuntimeError("æœªå®‰è£… transformersï¼Œè¯·åœ¨ç¯å¢ƒä¸­å®‰è£…åå†è¿è¡Œã€‚") from e

# ------------------------------
# æ—¥å¿—ï¼ˆé›†ä¸­å¼ï¼Œè‹¥ä¸å¯ç”¨åˆ™å›é€€åˆ°æ ‡å‡† loggingï¼‰
# ------------------------------
try:
    from kan.utils.logging import configure_logging, log_context
    import logging

    LOGGER = logging.getLogger("kan.pipelines.train_trainer")
except Exception:  # å…¼å®¹ä»“åº“å°šæœªè½ä½ `kan/utils/logging.py` çš„æƒ…å†µ
    import logging

    logging.basicConfig(
        level=logging.INFO, format="[%(asctime)s] %(levelname)s %(name)s | %(message)s"
    )
    LOGGER = logging.getLogger("kan.pipelines.train_trainer")

    def configure_logging(*args, **kwargs):  # type: ignore
        pass

    from contextlib import contextmanager

    @contextmanager
    def log_context(**kwargs):  # type: ignore
        yield


# ------------------------------
# å·¥å…·ï¼šYAML è¯»å–ä¸æ·±åˆå¹¶ã€ç‚¹å·è¦†ç›–
# ------------------------------
try:
    import yaml  # type: ignore
except Exception as e:  # pragma: no cover
    raise RuntimeError("ç¼ºå°‘ PyYAMLï¼Œè¯· `pip install pyyaml`ã€‚") from e


def _deep_update(
    base: MutableMapping[str, Any], other: Mapping[str, Any]
) -> MutableMapping[str, Any]:
    for k, v in other.items():
        if isinstance(v, Mapping) and isinstance(base.get(k), Mapping):
            _deep_update(base[k], v)  # type: ignore
        else:
            base[k] = copy.deepcopy(v)
    return base


def _read_yaml(path: Path) -> Dict[str, Any]:
    with path.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def _apply_overrides(cfg: MutableMapping[str, Any], overrides: Sequence[str]) -> None:
    for ov in overrides:
        if "=" not in ov:
            raise ValueError(f"è¦†ç›–é¡¹æ ¼å¼é”™è¯¯ï¼š{ov}ï¼Œåº”ä¸º key.subkey=VALUE")
        key, val = ov.split("=", 1)
        # è¯•å›¾å°†å­—é¢é‡è½¬ä¸º JSONï¼ˆæ•°å­—/å¸ƒå°”/æ•°ç»„/å¯¹è±¡ï¼‰
        try:
            val_conv = json.loads(val)
        except Exception:
            val_conv = val
        # ç‚¹å·èµ‹å€¼
        cur: MutableMapping[str, Any] = cfg
        parts = key.split(".")
        for p in parts[:-1]:
            if p not in cur or not isinstance(cur[p], MutableMapping):
                cur[p] = {}
            cur = cur[p]  # type: ignore
        cur[parts[-1]] = val_conv


# ------------------------------
# æ•°æ®åŠ è½½ä¸æ‰¹å¤„ç†ï¼ˆä¾èµ– kan.data.*ï¼‰
# ------------------------------


class RecordsDataset(Dataset):
    """æŒ‰éœ€ä» `kan.data.loaders` è¿”å›çš„ List[NewsRecord] æš´éœ²ç»™ Trainerã€‚

    æˆ‘ä»¬ä¸åœ¨æ­¤å¤„åšå¼ é‡åŒ–ï¼Œè€Œæ˜¯åœ¨ Collator ä¸­è°ƒç”¨ batcher ç»Ÿä¸€æ‰“åŒ…ï¼Œä¾¿äºçµæ´»è£å‰ª E/N/Tã€‚
    """

    def __init__(self, records: List[Mapping[str, Any]]):
        self.records = records

    def __len__(self) -> int:  # type: ignore[override]
        return len(self.records)

    def __getitem__(self, idx: int) -> Mapping[str, Any]:  # type: ignore[override]
        return self.records[idx]


class KANDataCollator:
    """è°ƒç”¨ `kan.data.batcher.Batcher` çš„ collate æ¥å£åœ¨ **collate_fn** é˜¶æ®µå®Œæˆæ‰“åŒ…ã€‚
    è¿™æ · Dataset ä»…æ‰¿æ‹…â€œæ ·æœ¬è®°å½•å®¹å™¨â€ï¼ŒCollator è´Ÿè´£æŠŠä¸€æ‰¹ `NewsRecord` â†’ model inputsã€‚
    """

    def __init__(self, batcher: Any):
        self.batcher = batcher

    def __call__(self, examples: List[Mapping[str, Any]]) -> Mapping[str, Any]:
        return self.batcher.collate(examples)


# ------------------------------
# KAN ç»„åˆæ¨¡å‹å°è£…ä¸º nn.Module ä¾› Trainer ä½¿ç”¨
# ------------------------------


class KANForNewsClassification(nn.Module):
    """æŠŠä¸‰è·¯ç¼–ç å™¨ + NE/NE2C + Head ç»„åˆä¸ºä¸€ä¸ªå¯è®­ç»ƒæ¨¡å—ã€‚

    çº¦å®šï¼š
    - `text_*` é”®æ¥è‡ª batcher çš„ `text_tok.*` æˆ– `text_vec`ï¼›
    - `ent_*` / `ctx_*` é”®æ¥è‡ª batcher çš„ `ent_ids/ctx_ids/...`ï¼›
    - è¿”å›å­—å…¸åŒ…å« `loss`ï¼ˆè‹¥æä¾› labelsï¼‰ä¸ `logits`ã€‚
    """

    def __init__(
        self,
        text_encoder: Any,
        entity_encoder: Optional[Any],
        context_encoder: Optional[Any],
        ne: Optional[Any],
        ne2c: Optional[Any],
        head: Any,
        use_q: bool = True,
        use_r: bool = True,
        num_labels: int = 2,
    ):
        super().__init__()
        self.text_encoder = text_encoder
        self.entity_encoder = entity_encoder
        self.context_encoder = context_encoder
        self.ne = ne
        self.ne2c = ne2c
        self.head = head
        self.use_q = use_q
        self.use_r = use_r
        self.num_labels = num_labels

    def forward(self, **batch) -> Mapping[str, torch.Tensor]:  # type: ignore[override]
        # 1) æ–‡æœ¬ç¼–ç  â†’ p
        if "text_tok" in batch:
            te_out = self.text_encoder(
                **batch["text_tok"]
            )  # æœŸæœ›ï¼š{sequence_output, pooled_output, attention_mask}
            p = te_out["pooled_output"]
            news_hidden = te_out.get("sequence_output")
            news_mask = te_out.get("attention_mask")
        elif "text_vec" in batch:  # å¥å‘é‡åç«¯ï¼ˆå¤‡ç”¨ï¼‰
            p = batch["text_vec"]
            news_hidden, news_mask = None, None
        else:
            raise ValueError("batch ç¼ºå°‘ text_tok æˆ– text_vec")

        q_prime = None
        r_prime = None
        ents_mask = None
        ctxs_mask = None

        # 2) å®ä½“/ä¸Šä¸‹æ–‡ç¼–ç ï¼ˆè‹¥å¯ç”¨ï¼‰
        if self.entity_encoder is not None and ("ent_ids" in batch):
            ent_out = self.entity_encoder(
                entity_ids=batch["ent_ids"],
                entity_mask=batch.get("ent_mask"),
                context_ids=batch.get("ctx_ids"),
                context_mask=batch.get("ctx_mask"),
            )
            q_prime = ent_out.get("entities_last_hidden")
            ents_mask = ent_out.get("entities_mask") or batch.get("ent_mask")
            # ä¸Šä¸‹æ–‡å¯èƒ½æ¥æºäº entity_encoder æˆ–ç‹¬ç«‹çš„ context_encoder
            r_prime = ent_out.get("contexts_last_hidden")
            ctxs_mask = ent_out.get("contexts_mask") or batch.get("ctx_mask")

        if self.context_encoder is not None and ("context_input_ids" in batch):
            ctx_out = self.context_encoder(
                context_input_ids=batch["context_input_ids"],
                context_attention_mask=batch["context_attention_mask"],
                context_token_type_ids=batch.get("context_token_type_ids"),
                contexts_mask=batch.get("ctx_mask"),
            )
            r_prime = ctx_out.get("contexts_last_hidden")
            ctxs_mask = ctx_out.get("contexts_mask") or batch.get("ctx_mask")

        # 3) çŸ¥è¯†æ³¨æ„åŠ›èåˆï¼ˆå¯é€‰æ‹©å…³é—­ï¼‰
        q = None
        r = None
        if self.use_q and self.ne is not None and (q_prime is not None):
            try:
                q_out = self.ne(
                    news={"last_hidden_state": news_hidden, "pooled_state": p},
                    entities={"last_hidden_state": q_prime},
                    news_mask=news_mask,
                    entity_mask=ents_mask,
                    return_weights=False,
                )
                q = q_out.get("pooled") if isinstance(q_out, dict) else q_out
            except NotImplementedError:
                # ç­¾åå­˜åœ¨ä½†ç®—æ³•æœªå®ç°æ—¶ï¼Œä¼˜é›…é€€åŒ–ï¼šè·³è¿‡ q
                q = None
        if (
            self.use_r
            and self.ne2c is not None
            and (q_prime is not None)
            and (r_prime is not None)
        ):
            try:
                r_out = self.ne2c(
                    news={"last_hidden_state": news_hidden, "pooled_state": p},
                    entities={"last_hidden_state": q_prime},
                    contexts_last_hidden=r_prime,
                    news_mask=news_mask,
                    entity_mask=ents_mask,
                    contexts_mask=ctxs_mask,
                    return_weights=False,
                )
                r = r_out.get("pooled") if isinstance(r_out, dict) else r_out
            except NotImplementedError:
                r = None

        # 4) Head åˆ†ç±»
        head_out = self.head(
            p=p if p is not None else None, q=q, r=r, labels=batch.get("labels")
        )
        # å…¼å®¹ï¼šhead å¯èƒ½è¿”å› {logits,(loss),...}
        loss = head_out.get("loss") if isinstance(head_out, Mapping) else None
        logits = head_out.get("logits") if isinstance(head_out, Mapping) else head_out
        return {"loss": loss, "logits": logits}


# ------------------------------
# ç»„ä»¶æ„å»ºï¼ˆRegistry ä¼˜å…ˆï¼Œé€€åŒ–åˆ°ç®€å•æ„é€ ï¼‰
# ------------------------------


def build_components(
    cfg: Mapping[str, Any],
) -> Tuple[Any, Optional[Any], Optional[Any], Optional[Any], Optional[Any], Any]:
    """ä»é…ç½®æ„å»º text/entity/context encodersã€NE/NE2Cã€Headã€‚
    ä¼˜å…ˆä½¿ç”¨ `kan.utils.registry.HUB`ï¼Œä¾¿äºæŒ‰ `type/name` è§£è€¦ï¼›è‹¥ä¸å¯ç”¨åˆ™å°è¯•ç›´æ¥ä»æ¨¡å—å¯¼å…¥é»˜è®¤æ„é€ å™¨ã€‚
    """
    # try registry hub
    try:
        from kan.utils.registry import HUB, build_from_config

        TEXT = HUB.get_or_create("text_encoder")
        ENTITY = HUB.get_or_create("entity_encoder")
        CONTEXT = HUB.get_or_create("context_encoder")
        ATT = HUB.get_or_create("attention")
        HEAD = HUB.get_or_create("head")

        te = build_from_config(cfg.get("text_encoder", {}), TEXT)
        ee = (
            build_from_config(cfg.get("entity_encoder", {}), ENTITY)
            if cfg.get("entity_encoder")
            else None
        )
        ce = (
            build_from_config(cfg.get("context_encoder", {}), CONTEXT)
            if cfg.get("context_encoder")
            else None
        )
        ne = (
            build_from_config(cfg.get("ne", {"type": "ne"}), ATT)
            if cfg.get("ne")
            else None
        )
        ne2c = (
            build_from_config(cfg.get("ne2c", {"type": "ne2c"}), ATT)
            if cfg.get("ne2c")
            else None
        )
        hd = build_from_config(cfg.get("head", {}), HEAD)
        return te, ee, ce, ne, ne2c, hd
    except Exception as e:
        LOGGER.warning("Registry æ„å»ºå¤±è´¥ï¼Œå°è¯•ç›´æ¥å¯¼å…¥æ¨¡å—ï¼š%s", e)
        # Fallbackï¼šç›´æ¥ import é¢„è®¾æ„é€ å‡½æ•°ï¼ˆè¦æ±‚ä»£ç å·²å®ç°ï¼‰
        from kan.modules.text_encoder import build_text_encoder
        from kan.modules.entity_encoder import build_entity_encoder
        from kan.modules.context_encoder import build_context_encoder
        from kan.modules.head import build_head

        te = build_text_encoder(cfg.get("text_encoder", {}))
        ee = (
            build_entity_encoder(cfg.get("entity_encoder", {}))
            if cfg.get("entity_encoder")
            else None
        )
        ce = (
            build_context_encoder(cfg.get("context_encoder", {}))
            if cfg.get("context_encoder")
            else None
        )
        ne = None
        ne2c = None
        try:
            from kan.modules.attention.ne import NEAttention  # type: ignore

            ne = NEAttention(**cfg.get("ne", {})) if cfg.get("ne") else None
        except Exception:
            pass
        try:
            from kan.modules.attention.ne2c import NE2CAttention  # type: ignore

            ne2c = NE2CAttention(**cfg.get("ne2c", {})) if cfg.get("ne2c") else None
        except Exception:
            pass
        hd = build_head(cfg.get("head", {}))
        return te, ee, ce, ne, ne2c, hd


# ------------------------------
# æŒ‡æ ‡ï¼šæ¡¥æ¥åˆ° kan.utils.metrics
# ------------------------------


def make_compute_metrics(problem_type: str = "single_label_classification"):
    from kan.utils.metrics import compute_classification_metrics

    def _fn(eval_pred):
        logits, labels = eval_pred
        if isinstance(logits, (list, tuple)):
            logits = logits[0]
        logits = torch.tensor(logits)
        labels = torch.tensor(labels)
        if problem_type == "single_label_classification":
            y_score = torch.softmax(logits, dim=-1).cpu().numpy()
            y_pred = y_score.argmax(axis=-1)
            return compute_classification_metrics(
                labels.cpu().numpy(), y_pred=y_pred, y_score=y_score
            )
        elif problem_type == "multilabel_classification":
            y_score = torch.sigmoid(logits).cpu().numpy()
            y_pred = (y_score >= 0.5).astype("i4")
            return compute_classification_metrics(
                labels.cpu().numpy(), y_pred=y_pred, y_score=y_score
            )
        else:  # regression
            from sklearn.metrics import mean_squared_error

            preds = logits.squeeze(-1).cpu().numpy()
            labels = labels.cpu().numpy()
            return {"mse": float(mean_squared_error(labels, preds))}

    return _fn


# ------------------------------
# ä¸»å…¥å£ï¼šç»„è£…é…ç½® â†’ æ•°æ® â†’ è®­ç»ƒå™¨ â†’ è®­ç»ƒ/è¯„ä¼°/ä¿å­˜
# ------------------------------


def run_from_configs(
    config_paths: Sequence[str], overrides: Sequence[str] = ()
) -> Path:
    """åˆå¹¶é…ç½®å¹¶è¿è¡Œè®­ç»ƒã€‚è¿”å›æœ¬æ¬¡ run çš„è¾“å‡ºç›®å½•ã€‚"""
    # 1) åˆå¹¶é…ç½®
    cfg: MutableMapping[str, Any] = {}
    for p in config_paths:
        cp = Path(p)
        assert cp.exists(), f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{cp}"
        _deep_update(cfg, _read_yaml(cp))
    _apply_overrides(cfg, list(overrides))

    # 2) è¾“å‡ºç›®å½•ä¸æ—¥å¿—
    now = time.strftime("%Y%m%d-%H%M%S")
    run_id = cfg.get("run_id") or f"{cfg.get('name', 'kan')}-{now}"
    out_dir = Path(cfg.get("output_dir", "runs")) / run_id
    out_dir.mkdir(parents=True, exist_ok=True)

    # é…ç½®é›†ä¸­å¼æ—¥å¿—
    log_dir = out_dir / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)
    try:
        configure_logging(log_dir=log_dir)
    except Exception:
        pass

    with log_context(run_id=str(run_id), stage="train", step=0):
        LOGGER.info("run_id=%s | è¾“å‡ºç›®å½•=%s", run_id, out_dir)
        (out_dir / "configs_merged.yaml").write_text(
            yaml.safe_dump(dict(cfg), allow_unicode=True), encoding="utf-8"
        )

        # 3) éšæœºæ€§ & è®¾å¤‡
        try:
            from kan.utils.seed import set_seed

            seed = int(cfg.get("seed", 42))
            set_seed(seed, deterministic=bool(cfg.get("deterministic", True)))
        except Exception:
            seed = int(cfg.get("seed", 42))
            hf_set_seed(seed)
        device = torch.device(
            "cuda"
            if torch.cuda.is_available() and cfg.get("device", "cuda") == "cuda"
            else "cpu"
        )
        LOGGER.info("device=%s cuda_available=%s", device, torch.cuda.is_available())

        # 4) æ•°æ®ï¼šåŠ è½½ â†’ è¯è¡¨/æ‰¹å¤„ç† â†’ Dataset/Collator
        from kan.data.loaders import loader_from_config, Dataset as NewsDataset  # type: ignore
        from kan.data.batcher import Batcher, BatcherConfig, TextConfig, EntityConfig, ContextConfig  # type: ignore

        data_cfg = cfg.get("data") or cfg  # å…¼å®¹ï¼šç›´æ¥æŠŠ data å­—æ®µé“ºåœ¨æ ¹éƒ¨
        loader = loader_from_config(data_cfg)
        # é¢„åŠ è½½è®­ç»ƒé›†ä»¥ä¾¿æ„å»ºè¯è¡¨
        train_records = loader.load_split(data_cfg.get("train_split", "train"))
        # è¯è¡¨/æ‰¹å¤„ç†å™¨ï¼ˆæ˜ç¡® CPU ä¸Šæ„å»ºï¼‰
        batcher = Batcher(
            BatcherConfig(
                text=TextConfig(**(data_cfg.get("batcher", {}).get("text", {}))),
                entity=EntityConfig(**(data_cfg.get("batcher", {}).get("entity", {}))),
                context=ContextConfig(
                    **(data_cfg.get("batcher", {}).get("context", {}))
                ),
                device="cpu",
            )
        )
        batcher.build_vocabs(train_records)
        # Dataset & Collator
        collator = KANDataCollator(batcher)
        ds_train = RecordsDataset(train_records)
        split_valid = data_cfg.get("validation_split", "validation")
        ds_valid = (
            RecordsDataset(loader.load_split(split_valid))
            if loader.has_split(split_valid)
            else None
        )
        split_test = data_cfg.get("test_split", "test")
        ds_test = (
            RecordsDataset(loader.load_split(split_test))
            if loader.has_split(split_test)
            else None
        )

        # 5) æ„å»ºç»„ä»¶å¹¶åŒ…æˆæ¨¡å‹
        te, ee, ce, ne, ne2c, head = build_components(cfg)
        model = KANForNewsClassification(
            text_encoder=te,
            entity_encoder=ee,
            context_encoder=ce,
            ne=ne,
            ne2c=ne2c,
            head=head,
            use_q=bool(cfg.get("head", {}).get("use_q", True)),
            use_r=bool(cfg.get("head", {}).get("use_r", True)),
            num_labels=int(cfg.get("head", {}).get("num_labels", 2)),
        )
        model.to(device)

        # 6) è®­ç»ƒå‚æ•°ï¼ˆè½¬ä¸º TrainingArgumentsï¼‰
        targs_dict = cfg.get("train", {})
        # å¿…å¡«é¡¹ï¼šè¾“å‡ºç›®å½•
        targs = TrainingArguments(
            output_dir=str(out_dir / "artifacts"),
            overwrite_output_dir=True,
            do_train=True,
            do_eval=ds_valid is not None,
            evaluation_strategy=targs_dict.get("evaluation_strategy", "epoch"),
            per_device_train_batch_size=int(targs_dict.get("batch_size", 8)),
            per_device_eval_batch_size=int(
                targs_dict.get("eval_batch_size", targs_dict.get("batch_size", 8))
            ),
            learning_rate=float(targs_dict.get("optimizer", {}).get("lr", 5e-5)),
            weight_decay=float(
                targs_dict.get("optimizer", {}).get("weight_decay", 0.0)
            ),
            num_train_epochs=float(targs_dict.get("max_epochs", 3)),
            gradient_accumulation_steps=int(targs_dict.get("grad_accum", 1)),
            logging_steps=int(targs_dict.get("logging", {}).get("every_n_steps", 50)),
            save_strategy=targs_dict.get("save_strategy", "epoch"),
            save_total_limit=int(targs_dict.get("save_total_limit", 3)),
            fp16=bool(cfg.get("fp16", False)),
            bf16=bool(cfg.get("bf16", False)),
            seed=int(seed),
            dataloader_num_workers=int(
                targs_dict.get("dataloader", {}).get("num_workers", 0)
            ),  # Windows å‹å¥½
            report_to=targs_dict.get("report_to", []),
        )

        # 7) Trainer
        compute_metrics = make_compute_metrics(
            problem_type=cfg.get("head", {}).get(
                "problem_type", "single_label_classification"
            )
        )
        trainer = Trainer(
            model=model,
            args=targs,
            train_dataset=ds_train,
            eval_dataset=ds_valid,
            data_collator=collator,
            compute_metrics=compute_metrics if ds_valid is not None else None,
        )

        # 8) è®­ç»ƒ
        train_result = trainer.train()
        (out_dir / "train_metrics.json").write_text(
            json.dumps(train_result.metrics, indent=2, ensure_ascii=False),
            encoding="utf-8",
        )
        trainer.save_model()  # ä¿å­˜æƒé‡

        # 9) è¯„ä¼°ï¼ˆvalid/testï¼‰ä¸é¢„æµ‹æŒä¹…åŒ–
        def _eval_and_save(name: str, dataset: Optional[Dataset]):
            if dataset is None:
                return
            metrics = trainer.evaluate(dataset=dataset)
            (out_dir / f"eval_{name}.json").write_text(
                json.dumps(metrics, indent=2, ensure_ascii=False), encoding="utf-8"
            )
            # é¢„æµ‹åˆ†æ•°ä¸æ ‡ç­¾
            preds = trainer.predict(dataset)
            logits = (
                preds.predictions
                if not isinstance(preds.predictions, (list, tuple))
                else preds.predictions[0]
            )
            y_score = torch.softmax(torch.tensor(logits), dim=-1).tolist()
            y_pred = torch.tensor(y_score).argmax(dim=-1).tolist()
            y_true = (
                preds.label_ids.tolist()
                if isinstance(preds.label_ids, (list, tuple))
                else preds.label_ids
            )
            rows = [
                {
                    "y_true": int(y_true[i]),
                    "y_pred": int(y_pred[i]),
                    "y_score": y_score[i],
                }
                for i in range(len(y_pred))
            ]
            (out_dir / f"pred_{name}.jsonl").write_text(
                "\n".join(json.dumps(r, ensure_ascii=False) for r in rows),
                encoding="utf-8",
            )

        _eval_and_save("validation", ds_valid)
        _eval_and_save("test", ds_test)

        LOGGER.info("è®­ç»ƒå®Œæˆï¼Œè¾“å‡ºä½äºï¼š%s", out_dir)
        return out_dir


# ------------------------------
# CLI å…¥å£ï¼ˆä¾› scripts/ è°ƒç”¨ï¼Œäº¦å¯ç›´æ¥è¿è¡Œï¼‰
# ------------------------------


def build_argparser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(description="Train KAN with ğŸ¤— Trainer (pipeline)")
    p.add_argument(
        "-c",
        "--config",
        nargs="+",
        required=True,
        help="YAML é…ç½®æ–‡ä»¶åˆ—è¡¨ï¼Œåè€…è¦†ç›–å‰è€…",
    )
    p.add_argument(
        "-o",
        "--override",
        nargs="*",
        default=[],
        help="ç‚¹å·è¦†ç›–ï¼Œå¦‚ head.num_labels=2 optimizer.lr=3e-5",
    )
    return p


def main(argv: Optional[Sequence[str]] = None) -> None:  # pragma: no cover
    args = build_argparser().parse_args(argv)
    run_from_configs(args.config, args.override)


if __name__ == "__main__":  # pragma: no cover
    main()
