# ContextEncoder（实体上下文序列编码器）配置
# 对齐论文：单层 Transformer，隐藏维/头数与文本/实体侧一致。

name: transformer_context_default

hidden_size: 100
num_layers: 1
num_heads: 4
ffn_dim: 2048
dropout: 0.5

attn_dropout: 0.1
activation: relu

max_seq_len: 64          # 每实体上下文截断长度
layer_norm_eps: 1e-5
