# 通用训练基座（被各数据集入口“复用/覆盖”）
run:
  run_id: "${now:%Y%m%d-%H%M%S}"
  output_dir: ".runs/${run_id}"
  seed: 2025
  deterministic: true
  device: "cuda"            # 自动选 CUDA；无卡则脚本内回退 CPU
  mixed_precision: "bf16"   # Ampere 及以上建议 bf16
  compile: true             # torch.compile(True)
  log_cfg: null             # 为空则走 kan.utils.logging 的默认配置

data:
  # 由各数据集入口覆盖，如：configs/data/politifact.yaml
  config: null
  # 切分策略（与论文一致）：先固定 10% 验证集（调参），再对其余做 K 折
  split:
    validation_holdout: 0.10
    kfold:
      folds: 5
      stratified: true
      shuffle: true
      seed: 2025

model:
  # 指向模型与融合配置（各数据集入口可复用/覆盖）
  text_encoder: "configs/model/transformer_text.yaml"
  entity_encoder: "configs/model/transformer_entity.yaml"
  context_encoder: "configs/model/transformer_context.yaml"
  fusion:
    ne: "configs/fusion/ne.yaml"
    ne2c: "configs/fusion/ne2c.yaml"
  head: "configs/model/head.yaml"

train:
  epochs: 30
  batch_size: 32
  grad_accum_steps: 1
  max_grad_norm: 1.0
  optimizer:
    type: "adam"            # 论文使用 Adam
    lr: 3.0e-4              # 如走预训练文本编码器，可在数据集入口下调至 2e-5~5e-5
    betas: [0.9, 0.999]
    weight_decay: 1.0e-2
  scheduler:
    type: "cosine_with_warmup"
    warmup_ratio: 0.06
  early_stopping:
    metric: "f1_macro"
    mode: "max"
    patience: 5
    min_delta: 0.0
  evaluation:
    metrics: ["accuracy", "precision_macro", "recall_macro", "f1_macro", "roc_auc_ovr"]
    eval_on: "epoch"        # or "steps"
    save_top_k: 3
    monitor: "f1_macro"
    mode: "max"

dataloader:
  num_workers: 4
  pin_memory: true
  persistent_workers: true
